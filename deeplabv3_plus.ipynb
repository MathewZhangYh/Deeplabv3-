{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9796335",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from nets.deeplabv3_plus import DeepLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fee4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## partition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f8a833",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 划分比例\n",
    "train_percent       = 0.85\n",
    "dataset_path      = 'weizmann_horse_db'\n",
    "random.seed(0)\n",
    "\n",
    "segfilepath     = os.path.join(dataset_path, 'mask')\n",
    "saveBasePath    = os.path.join(dataset_path, 'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed65abee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp_seg = os.listdir(segfilepath)\n",
    "total_seg = []\n",
    "for seg in temp_seg:\n",
    "    if seg.endswith(\".png\"):\n",
    "        total_seg.append(seg)\n",
    "\n",
    "num = len(total_seg)  \n",
    "list = range(num)  \n",
    "tv = int(num)\n",
    "tr = int(tv*train_percent)  \n",
    "trainval = random.sample(list,tv)  \n",
    "train = random.sample(trainval,tr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6a687a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 277\n"
     ]
    }
   ],
   "source": [
    "#划分结果存入 weizmann_horse_db/datasets\n",
    "ftrainval = open(os.path.join(saveBasePath,'trainval.txt'), 'w')  \n",
    "ftest = open(os.path.join(saveBasePath,'test.txt'), 'w')  \n",
    "ftrain = open(os.path.join(saveBasePath,'train.txt'), 'w')  \n",
    "fval = open(os.path.join(saveBasePath,'val.txt'), 'w')  \n",
    "\n",
    "for i in list:  \n",
    "    name = total_seg[i][:-4]+'\\n'  \n",
    "    if i in trainval:  \n",
    "        ftrainval.write(name)  \n",
    "        if i in train:  \n",
    "            ftrain.write(name)  \n",
    "        else:  \n",
    "            fval.write(name)  \n",
    "    else:  \n",
    "        ftest.write(name)  \n",
    "\n",
    "ftrainval.close()  \n",
    "ftrain.close()  \n",
    "fval.close()  \n",
    "ftest.close()\n",
    "\n",
    "print(\"train size\",tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae0ebe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load model and set logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7837b328",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Cuda = True\n",
    "num_classes = 2\n",
    "model_path = \"model/deeplabv3+_model.pth\"\n",
    "dataset_path = 'weizmann_horse_db'\n",
    "save_dir = 'logs'\n",
    "\n",
    "input_shape = [512, 512]\n",
    "\n",
    "Epoch = 1\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11e7f65",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of keys which fail loading: 0\n"
     ]
    }
   ],
   "source": [
    "# 初始话模型权重\n",
    "def weights_init(net):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    net.apply(init_func)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DeepLab(num_classes=num_classes)\n",
    "weights_init(model)\n",
    "\n",
    "# 根据预训练权重的Key和模型的Key进行加载\n",
    "model_dict      = model.state_dict()\n",
    "pretrained_dict = torch.load(model_path, map_location = device)\n",
    "\n",
    "load_key, no_load_key, temp_dict = [], [], {}\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
    "        temp_dict[k] = v\n",
    "        load_key.append(k)\n",
    "    else:\n",
    "        no_load_key.append(k)\n",
    "model_dict.update(temp_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "print('The numbers of keys which fail loading:',len(no_load_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cf4e48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LossHistory():\n",
    "    def __init__(self, log_dir, model, input_shape):\n",
    "        self.log_dir = log_dir\n",
    "        self.losses = []\n",
    "        self.val_loss = []\n",
    "\n",
    "        os.makedirs(self.log_dir)\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "        try:\n",
    "            dummy_input = torch.randn(2, 3, input_shape[0], input_shape[1])\n",
    "            self.writer.add_graph(model, dummy_input)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def append_loss(self, epoch, loss, val_loss):\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        self.losses.append(loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "\n",
    "        self.writer.add_scalar('loss', loss, epoch)\n",
    "        self.writer.add_scalar('val_loss', val_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47621cd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 日志保存\n",
    "time_str = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "log_dir = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "loss_history = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "model_train = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3265d26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Cuda:\n",
    "    model_train = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    model_train = model_train.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d331aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c7c3c87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def cvtColor(image):\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
    "        return image\n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba4c864",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 载入数据集，对训练图数据增强\n",
    "class DeeplabDataset():\n",
    "    def __init__(self, annotation_lines, input_shape, num_classes, train, dataset_path):\n",
    "        super(DeeplabDataset, self).__init__()\n",
    "        self.annotation_lines = annotation_lines\n",
    "        self.length = len(annotation_lines)\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.train = train\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annotation_line = self.annotation_lines[index]\n",
    "        name = annotation_line.split()[0]\n",
    "\n",
    "        # 从文件中读取图像\n",
    "        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, \"horse\"), name + \".jpg\"))\n",
    "        png = Image.open(os.path.join(os.path.join(self.dataset_path, \"mask\"), name + \".png\"))\n",
    "        # 数据增强\n",
    "        jpg, png = self.get_random_data(jpg, png, self.input_shape, random=self.train)\n",
    "\n",
    "        jpg = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2, 0, 1])\n",
    "        png = np.array(png)\n",
    "        png[png >= self.num_classes] = self.num_classes\n",
    "        # 转化成one_hot的形式 ,在这里需要+1是因为数据集有些标签具有白边部分, 我们需要将白边部分进行忽略，+1的目的是方便忽略。\n",
    "        seg_labels = np.eye(self.num_classes + 1)[png.reshape([-1])]\n",
    "        seg_labels = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes + 1))\n",
    "\n",
    "        return jpg, png, seg_labels\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.3, random=True):\n",
    "        image = cvtColor(image)\n",
    "        label = Image.fromarray(np.array(label))\n",
    "        # 获得图像的高宽与目标高宽\n",
    "        iw, ih = image.size\n",
    "        h, w = input_shape\n",
    "\n",
    "        if not random:\n",
    "            iw, ih = image.size\n",
    "            scale = min(w / iw, h / ih)\n",
    "            nw = int(iw * scale)\n",
    "            nh = int(ih * scale)\n",
    "\n",
    "            image = image.resize((nw, nh), Image.BICUBIC)\n",
    "            new_image = Image.new('RGB', [w, h], (128, 128, 128))\n",
    "            new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
    "\n",
    "            label = label.resize((nw, nh), Image.NEAREST)\n",
    "            new_label = Image.new('L', [w, h], (0))\n",
    "            new_label.paste(label, ((w - nw) // 2, (h - nh) // 2))\n",
    "            return new_image, new_label\n",
    "\n",
    "        # 对图像进行缩放并且进行长和宽的扭曲\n",
    "        new_ar = iw / ih * self.rand(1 - jitter, 1 + jitter) / self.rand(1 - jitter, 1 + jitter)\n",
    "        scale = self.rand(0.25, 2)\n",
    "        if new_ar < 1:\n",
    "            nh = int(scale * h)\n",
    "            nw = int(nh * new_ar)\n",
    "        else:\n",
    "            nw = int(scale * w)\n",
    "            nh = int(nw / new_ar)\n",
    "        image = image.resize((nw, nh), Image.BICUBIC)\n",
    "        label = label.resize((nw, nh), Image.NEAREST)\n",
    "\n",
    "        # 翻转图像\n",
    "        flip = self.rand() < .5\n",
    "        if flip:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # 将图像多余的部分加上灰条\n",
    "        dx = int(self.rand(0, w - nw))\n",
    "        dy = int(self.rand(0, h - nh))\n",
    "        new_image = Image.new('RGB', (w, h), (128, 128, 128))\n",
    "        new_label = Image.new('L', (w, h), (0))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        new_label.paste(label, (dx, dy))\n",
    "        image = new_image\n",
    "        label = new_label\n",
    "\n",
    "        image_data = np.array(image, np.uint8)\n",
    "\n",
    "        # 高斯模糊\n",
    "        blur = self.rand() < 0.25\n",
    "        if blur:\n",
    "            image_data = cv2.GaussianBlur(image_data, (5, 5), 0)\n",
    "\n",
    "        # 旋转\n",
    "        rotate = self.rand() < 0.25\n",
    "        if rotate:\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation = np.random.randint(-10, 11)\n",
    "            M = cv2.getRotationMatrix2D(center, -rotation, scale=1)\n",
    "            image_data = cv2.warpAffine(image_data, M, (w, h), flags=cv2.INTER_CUBIC, borderValue=(128, 128, 128))\n",
    "            label = cv2.warpAffine(np.array(label, np.uint8), M, (w, h), flags=cv2.INTER_NEAREST, borderValue=(0))\n",
    "\n",
    "        # 对图像进行色域变换 ,计算色域变换的参数\n",
    "        r = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
    "        # 将图像转到HSV上\n",
    "        hue, sat, val = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
    "        dtype = image_data.dtype\n",
    "        # 应用变换\n",
    "        x = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        return image_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1465d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    images = []\n",
    "    pngs = []\n",
    "    seg_labels = []\n",
    "    for img, png, labels in batch:\n",
    "        images.append(img)\n",
    "        pngs.append(png)\n",
    "        seg_labels.append(labels)\n",
    "    images = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
    "    pngs = torch.from_numpy(np.array(pngs)).long()\n",
    "    seg_labels = torch.from_numpy(np.array(seg_labels)).type(torch.FloatTensor)\n",
    "    return images, pngs, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ab1a2a-3f96-4db9-92ba-d31f5ce6c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_path, \"datasets/train.txt\"),\"r\") as f:\n",
    "        train_lines = f.readlines()\n",
    "with open(os.path.join(dataset_path, \"datasets/val.txt\"),\"r\") as f:\n",
    "        val_lines = f.readlines()\n",
    "num_train = len(train_lines)\n",
    "num_val   = len(val_lines)\n",
    "\n",
    "train_dataset = DeeplabDataset(train_lines, input_shape, num_classes, True, dataset_path)\n",
    "val_dataset = DeeplabDataset(val_lines, input_shape, num_classes, False, dataset_path)\n",
    "\n",
    "train_set = DataLoader(train_dataset, shuffle = True, batch_size = batch_size, pin_memory=True,\n",
    "                 drop_last = True, collate_fn = collate_function)\n",
    "val_set = DataLoader(val_dataset  , shuffle = True, batch_size = batch_size, pin_memory=True,\n",
    "                     drop_last = True, collate_fn = collate_function)\n",
    "# 判断每一个epoch的长度\n",
    "epoch_step = num_train // batch_size\n",
    "epoch_step_val = num_val // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c1d9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e88d3294",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr_scheduler(lr, min_lr, total_iters):\n",
    "    warmup_iters_ratio = 0.1\n",
    "    warmup_lr_ratio = 0.1\n",
    "    no_aug_iter_ratio = 0.3\n",
    "    def warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters):\n",
    "        if iters <= warmup_total_iters:\n",
    "            lr = (lr - warmup_lr_start) * pow(iters / float(warmup_total_iters), 2) + warmup_lr_start\n",
    "        elif iters >= total_iters - no_aug_iter:\n",
    "            lr = min_lr\n",
    "        else:\n",
    "            lr = min_lr + 0.5 * (lr - min_lr) * (\n",
    "                1.0 + math.cos(math.pi* (iters - warmup_total_iters) / (total_iters - warmup_total_iters - no_aug_iter))\n",
    "            )\n",
    "        return lr\n",
    "\n",
    "    warmup_total_iters = min(max(warmup_iters_ratio * total_iters, 1), 3)\n",
    "    warmup_lr_start = max(warmup_lr_ratio * lr, 1e-6)\n",
    "    no_aug_iter = min(max(no_aug_iter_ratio * total_iters, 1), 15)\n",
    "    func = partial(warm_cos_lr, lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter)\n",
    "    return func\n",
    "\n",
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515ef9c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "Init_lr = 7e-3\n",
    "Min_lr = Init_lr * 0.01\n",
    "\n",
    "nbs  = 16\n",
    "lr_limit_max = 1e-1\n",
    "lr_limit_min = 5e-4\n",
    "# 根据batch_size，自适应调整学习率\n",
    "Init_lr_fit = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "Min_lr_fit = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "# SGD优化器\n",
    "optimizer = optim.SGD(model.parameters(), Init_lr_fit, momentum = momentum, nesterov=True, weight_decay = weight_decay)\n",
    "# 获得学习率下降的公式\n",
    "lr_scheduler_func = get_lr_scheduler(Init_lr_fit, Min_lr_fit, Epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e08827",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ca6ccf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算 交叉熵损失\n",
    "def CE_Loss(inputs, target, cls_weights, num_classes=2):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    temp_target = target.view(-1)\n",
    "\n",
    "    CE_loss  = nn.CrossEntropyLoss(weight=cls_weights, ignore_index=num_classes)(temp_inputs, temp_target)\n",
    "    return CE_loss\n",
    "\n",
    "# 计算Dice损失\n",
    "def Dice_loss(inputs, target, beta=1, smooth = 1e-5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #   计算dice loss\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1] , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    dice_loss = 1 - torch.mean(score)\n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d5ff186",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit_one_epoch(model_train, model, loss_history, optimizer, epoch, Epoch, epoch_step, epoch_step_val,\n",
    "                  train_set, val_set, cuda , num_classes, save_dir):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    cls_weights = np.ones([num_classes], np.float32)\n",
    "    pbar = tqdm(total=epoch_step,desc=f'Epoch(train) {epoch + 1}/{Epoch}',postfix=dict)\n",
    "    model_train.train()\n",
    "    for iteration, batch in enumerate(train_set):\n",
    "        if iteration >= epoch_step:\n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs = imgs.cuda()\n",
    "                pngs = pngs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                weights = weights.cuda()\n",
    "        #   清零梯度\n",
    "        optimizer.zero_grad()\n",
    "        #   前向传播\n",
    "        outputs = model_train(imgs)\n",
    "        #   计算损失\n",
    "        loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "        loss_dice = Dice_loss(outputs, labels)\n",
    "        loss = loss + loss_dice\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix(**{'train_loss': train_loss / (iteration + 1)})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    pbar = tqdm(total=epoch_step_val, desc=f'Epoch(valid) {epoch + 1}/{Epoch}',postfix=dict)\n",
    "    model_train.eval()\n",
    "    for iteration, batch in enumerate(val_set):\n",
    "        if iteration >= epoch_step_val:\n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs = imgs.cuda()\n",
    "                pngs = pngs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                weights = weights.cuda()\n",
    "            #   前向传播\n",
    "            outputs = model_train(imgs)\n",
    "            #   计算损失\n",
    "            loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "            loss_dice = Dice_loss(outputs, labels)\n",
    "            loss  = loss + loss_dice\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pbar.set_postfix(**{'val_loss': val_loss / (iteration + 1)})\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    loss_history.append_loss(epoch + 1, train_loss / epoch_step, val_loss / epoch_step_val)\n",
    "    #   保存权值\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3cc096-ecc1-442d-b601-659edf7f2a21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch(train) 1/1: 100%|███████████████████████████████████████████████| 69/69 [00:31<00:00,  2.20it/s, train_loss=0.12]\n",
      "Epoch(valid) 1/1: 100%|███████████████████████████████████████████████| 12/12 [00:02<00:00,  4.84it/s, val_loss=0.0974]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(Epoch):\n",
    "    set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "    fit_one_epoch(model_train, model, loss_history, optimizer, epoch, Epoch, epoch_step, epoch_step_val,\n",
    "                  train_set, val_set, Cuda, num_classes, save_dir)\n",
    "loss_history.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cba7fb-3d61-40cd-acda-13cddd6e6f58",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## generate validation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a627c1b-bb1c-4b78-babe-384f65209417",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 对输入图像进行resize\n",
    "def resize_image(image, size):\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, nw, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a92fee4-c028-4c16-a0df-5bd16814321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成validation mask\n",
    "class Detection(object):\n",
    "    _defaults = {\"model_path\" : 'logs/best_model.pth',\"num_classes\" : 2,\n",
    "                 \"input_shape\" : [512, 512],\"cuda\" : True,\n",
    "    }\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults)\n",
    "        for name, value in kwargs.items():\n",
    "            setattr(self, name, value)\n",
    "        #   画框的颜色\n",
    "        self.colors = [ (0, 0, 0), (128, 0, 0)]\n",
    "        self.generate()\n",
    "    #   获得所有的分类\n",
    "    def generate(self):\n",
    "        #   载入模型与权值\n",
    "        self.net = DeepLab(num_classes=self.num_classes)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.net.load_state_dict(torch.load(self.model_path, map_location=device))\n",
    "        self.net = self.net.eval()\n",
    "\n",
    "        if self.cuda:\n",
    "            self.net = nn.DataParallel(self.net)\n",
    "            self.net = self.net.cuda()\n",
    "\n",
    "    def get_detection(self, image):\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        image = cvtColor(image)\n",
    "        orininal_h = np.array(image).shape[0]\n",
    "        orininal_w = np.array(image).shape[1]\n",
    "\n",
    "        image_data, nw, nh = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #   添加上batch_size维度\n",
    "        image_data = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            pr = self.net(images)[0]\n",
    "            #   取出每一个像素点的种类\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #   将灰条部分截取掉\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #   进行图片的resize\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #   取出每一个像素点的种类\n",
    "            pr = pr.argmax(axis=-1)\n",
    "\n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a539b-7716-4dd9-8b05-e298afb87035",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## from hist to miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68bc8814-c3ae-42ee-8c5c-3917c99078b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设标签宽W，长H\n",
    "def fast_hist(a, b, n):\n",
    "    # a是转化成一维数组的标签，形状(H×W,)；b是转化成一维数组的预测结果，形状(H×W,)\n",
    "    k = (a >= 0) & (a < n)\n",
    "    # np.bincount计算了从0到n**2-1这n**2个数中每个数出现的次数，返回值形状(n, n)\n",
    "    # 返回中，写对角线上的为分类正确的像素点\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "def per_class_iu(hist):\n",
    "    return np.diag(hist) / np.maximum((hist.sum(1) + hist.sum(0) - np.diag(hist)), 1)\n",
    "\n",
    "def cal_miou(gt_dir, pred_dir, png_name_list, num_classes):\n",
    "    # 创建一个全是0的矩阵，是一个混淆矩阵\n",
    "    hist = np.zeros((num_classes, num_classes))\n",
    "    # 获得验证集标签路径列表，获得验证集图像分割结果路径列表，方便直接读取\n",
    "    gt_imgs = [os.path.join(gt_dir, x + \".png\") for x in png_name_list]\n",
    "    pred_imgs = [os.path.join(pred_dir, x + \".png\") for x in png_name_list]\n",
    "    # 读取每一个（图片-标签）对\n",
    "    for ind in range(len(gt_imgs)):\n",
    "        # 读取一张图像分割结果，转化成numpy数组\n",
    "        pred = np.array(Image.open(pred_imgs[ind]))\n",
    "        # 读取一张对应的标签，转化成numpy数组\n",
    "        label = np.array(Image.open(gt_imgs[ind]))\n",
    "        # 如果图像分割结果与标签的大小不一样，这张图片就不计算\n",
    "        if len(label.flatten()) != len(pred.flatten()):  \n",
    "            print(\n",
    "                'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}'.format(\n",
    "                    len(label.flatten()), len(pred.flatten()), gt_imgs[ind],\n",
    "                    pred_imgs[ind]))\n",
    "            continue\n",
    "        # 对一张图片计算2×2的hist矩阵，并累加\n",
    "        hist += fast_hist(label.flatten(), pred.flatten(), num_classes)\n",
    "    # 计算所有验证集图片的逐类别mIoU值\n",
    "    miou = per_class_iu(hist)\n",
    "    # 在所有验证集图像上求所有类别平均的mIoU值，计算时忽略NaN值\n",
    "    return miou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf89fca-f887-4069-8e96-3bdfd4088022",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## from mask to boundary iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a79039e-52e8-4e05-8330-d8556fe706b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#mask--->boundary\n",
    "def mask_to_boundary(mask, dilation_ratio=0.02):\n",
    "    h, w = mask.shape\n",
    "    img_diag = np.sqrt(h ** 2 + w ** 2) # 计算图像对角线长度\n",
    "    dilation = int(round(dilation_ratio * img_diag))\n",
    "    if dilation < 1:\n",
    "        dilation = 1\n",
    "    # Pad image so mask truncated by the image border is also considered as boundary.\n",
    "    new_mask = cv2.copyMakeBorder(mask, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)\n",
    "    kernel = np.ones((3, 3), dtype=np.uint8)\n",
    "    new_mask_erode = cv2.erode(new_mask, kernel, iterations=dilation)   \n",
    "    # 因为之前向四周填充了0, 故而这里不再需要四周\n",
    "    mask_erode = new_mask_erode[1 : h + 1, 1 : w + 1]   \n",
    "    # G_d intersects G in the paper.\n",
    "    return mask - mask_erode\n",
    "\n",
    "#计算boundary iou\n",
    "def boundary_iou(gt, dt, dilation_ratio=0.02):\n",
    "    gt_boundary = mask_to_boundary(gt, dilation_ratio)\n",
    "    dt_boundary = mask_to_boundary(dt, dilation_ratio)\n",
    "    intersection = ((gt_boundary * dt_boundary) > 0).sum()\n",
    "    union = ((gt_boundary + dt_boundary) > 0).sum()\n",
    "    if union < 1:\n",
    "    \treturn 0\n",
    "    boundary_iou = intersection / union\n",
    "    return boundary_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215cf0c-47b8-42ed-8495-15c2d32a8328",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## calculate m_iou, boundary iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e03b8d-f00d-40e8-b958-39bba8baa7ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "===> MIoU:         93.36\n",
      "===> boundary iou: 71.49\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "#   指向数据集所在的文件夹\n",
    "dataset_path = 'weizmann_horse_db'\n",
    "\n",
    "image_ids = open(os.path.join(dataset_path, \"datasets/val.txt\"),'r').read().splitlines()\n",
    "gt_dir = os.path.join(dataset_path, \"mask/\")\n",
    "pred_dir = \"detection\"\n",
    "\n",
    "boundary_iou_list = []\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "\n",
    "Detect = Detection()\n",
    "\n",
    "for image_id in image_ids:\n",
    "    image_path = os.path.join(dataset_path, \"horse/\"+image_id+\".jpg\")\n",
    "    image = Image.open(image_path)\n",
    "    image = Detect.get_detection(image)\n",
    "    image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "    \n",
    "    gt_path = os.path.join(dataset_path, \"mask/\"+image_id+\".png\")\n",
    "    img_gt = Image.open(gt_path)\n",
    "    gt = np.array(img_gt)\n",
    "    \n",
    "    gt_path = os.path.join(pred_dir, image_id+\".png\")\n",
    "    img_dt = Image.open(gt_path)\n",
    "    dt = np.array(img_dt)\n",
    "    \n",
    "    b_iou = boundary_iou(gt, dt, dilation_ratio=0.02)\n",
    "    boundary_iou_list.append(b_iou)\n",
    "miou = cal_miou(gt_dir, pred_dir, image_ids, num_classes)\n",
    "print(\"----------------------------\")\n",
    "print('===> MIoU:         ' + str(round(np.nanmean(miou) * 100, 2)))\n",
    "print('===> boundary iou: ' + str(round(np.nanmean(boundary_iou_list) * 100, 2)))\n",
    "print(\"----------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1_2]",
   "language": "python",
   "name": "conda-env-pytorch1_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
